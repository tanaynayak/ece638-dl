{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Perceptron (40 points)\n",
    "\n",
    "In this problem, you will classify handwritten digits using the MNIST Database.\n",
    "Please download the four files found there, these will be used for this homework.\n",
    "To reduce computation time, please use only the first 20,000 training images/labels and only the first\n",
    "2,000 testing images/labels.\n",
    "\n",
    "Read in the data from files. Each image is 28 x 28 pixels. An image can be viewed as a\n",
    "784-dimensional vector of pixel intensities. For each image, append a `1` to the beginning\n",
    "of each vector; this will act as an intercept term (for bias). Use the gradient descent\n",
    "algorithm for Perceptron derived in class, to classify given `x ∈ R^785` whether a digit’s label\n",
    "is `k` or if it is some “other” digit, i.e. `k ≠ {0, ..., 9}`. For instance, if you are classifying “2”,\n",
    "you would designate `y = 2` as the positive class, and all other digits as the negative class.\n",
    "For each image, do this two-way classification for all 10 digits. You will train 10 perceptrons\n",
    "that will collectively learn to classify the handwritten digits in the MNIST dataset. Each\n",
    "perceptron will have 785 inputs and one output.\n",
    "\n",
    "1. Report the test accuracy for each of the 10 two-way classifications on the test set.\n",
    "\n",
    "2. For each image in the test set, report the overall test accuracy. An example will be\n",
    "considered labeled correctly if the perceptron classification of the true label has the\n",
    "highest probability. So for instance, if the true label was `{2}` for an image, you would\n",
    "count it as correctly classified if the perceptron test of `{2}` vs. `{0,1,3,4,5,6,7,8,9}`\n",
    "had the highest probability of all the 10 2-way classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 785), (20000,), (2000, 785), (2000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the dataset files\n",
    "files = {\n",
    "    'train_images': 'data/train-images.idx3-ubyte',\n",
    "    'train_labels': 'data/train-labels.idx1-ubyte',\n",
    "    'test_images': 'data/t10k-images.idx3-ubyte',\n",
    "    'test_labels': 'data/t10k-labels.idx1-ubyte'\n",
    "}\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "# Preparing the dataset\n",
    "def prepare_dataset(files, limit_train=20000, limit_test=2000):\n",
    "    train_images = read_idx(files['train_images'])\n",
    "    train_labels = read_idx(files['train_labels'])\n",
    "    test_images = read_idx(files['test_images'])\n",
    "    test_labels = read_idx(files['test_labels'])\n",
    "    \n",
    "    train_images = train_images[:limit_train]\n",
    "    train_labels = train_labels[:limit_train]\n",
    "    test_images = test_images[:limit_test]\n",
    "    test_labels = test_labels[:limit_test]\n",
    "    \n",
    "    train_images = train_images.reshape(limit_train, 784)\n",
    "    test_images = test_images.reshape(limit_test, 784)\n",
    "    \n",
    "    train_images = np.insert(train_images, 0, 1, axis=1)\n",
    "    test_images = np.insert(test_images, 0, 1, axis=1)\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = prepare_dataset(files)\n",
    "\n",
    "(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f31e601560744d58b1d414f9a899aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training perceptrons for each digit:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85df5f68902648428b99e4bbeecd0ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdb650838274684ad533cc837d0c01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b61c10c7a744eb90581a7b8de3c52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 2:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a78794dd754d8c8606b210d4e96be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 3:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc1a05075e744b59d646ce4d8439067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 4:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff2e0a3f56c41578d12bb8a6307a65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 5:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c053a3435a474273822a6a789bfd2f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 6:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f54f7b68634ef992f6ac9a1e228db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 7:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f854f7ea9846eb936d73d2c7593a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 8:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc655cb3f954822b596d7c7f5839211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Perceptron for digit 9:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of perceptrons trained: 10\n",
      "Shape of each perceptron weight vector: (785,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_perceptron(digit, X, y, learning_rate=1.0, epochs=1000):\n",
    "    \"\"\"\n",
    "    Trains a perceptron model using the given features and target labels.\n",
    "    Args:\n",
    "    X (numpy.ndarray): The feature vectors.\n",
    "    y (numpy.ndarray): The target labels.\n",
    "    learning_rate (float): The learning rate for weight updates.\n",
    "    epochs (int): The number of times to run through the training data.\n",
    "    Returns:\n",
    "    numpy.ndarray: The weights of the trained perceptron model.\n",
    "    \"\"\"\n",
    "    weights = np.zeros(X.shape[1])\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Perceptron for digit \" + str(digit)):\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction = np.dot(X[i], weights) >= 0\n",
    "            if (prediction != y[i]):\n",
    "                weights += learning_rate * (y[i] - prediction) * X[i]\n",
    "    return weights\n",
    "\n",
    "def train_10_perceptrons(X_train, y_train):\n",
    "    perceptron_weights = []\n",
    "    for digit in tqdm(range(10), desc=\"Training perceptrons for each digit\"):\n",
    "        y_binary = (y_train == digit).astype(int)\n",
    "        weights = train_perceptron(digit, X_train, y_binary)\n",
    "        perceptron_weights.append(weights)\n",
    "    return perceptron_weights\n",
    "\n",
    "perceptron_weights = train_10_perceptrons(train_images, train_labels)\n",
    "\n",
    "print(f'Number of perceptrons trained: {len(perceptron_weights)}')\n",
    "print(f'Shape of each perceptron weight vector: {perceptron_weights[0].shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for digit 0: 0.99\n",
      "Accuracy for digit 1: 0.99\n",
      "Accuracy for digit 2: 0.96\n",
      "Accuracy for digit 3: 0.96\n",
      "Accuracy for digit 4: 0.97\n",
      "Accuracy for digit 5: 0.97\n",
      "Accuracy for digit 6: 0.97\n",
      "Accuracy for digit 7: 0.96\n",
      "Accuracy for digit 8: 0.90\n",
      "Accuracy for digit 9: 0.94\n"
     ]
    }
   ],
   "source": [
    "def two_way_classification_accuracy(weights, X, y, positive_class):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a two-way classification for a given positive class.\n",
    "    \n",
    "    Args:\n",
    "    weights (numpy.ndarray): The weights of the perceptron.\n",
    "    X (numpy.ndarray): The feature vectors of the test set.\n",
    "    y (numpy.ndarray): The true labels of the test set.\n",
    "    positive_class (int): The digit considered as the positive class.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy of the two-way classification.\n",
    "    \"\"\"\n",
    "    predictions = np.dot(X, weights) >= 0\n",
    "    true_positive_class = y == positive_class\n",
    "    accuracy = np.mean(predictions == true_positive_class)\n",
    "    return accuracy\n",
    "\n",
    "two_way_accuracies = []\n",
    "for digit in range(10):\n",
    "    weights = perceptron_weights[digit]\n",
    "    accuracy = two_way_classification_accuracy(weights, test_images, test_labels, digit)\n",
    "    two_way_accuracies.append(accuracy)\n",
    "    print(f'Accuracy for digit {digit}: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall test accuracy is: 0.84\n"
     ]
    }
   ],
   "source": [
    "def predict_perceptron(weights, X):\n",
    "    \"\"\"Make predictions using learned weights\"\"\"\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "def calculate_overall_accuracy(perceptron_weights, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate the overall accuracy on the test set where an image is considered \n",
    "    correctly labeled if the perceptron for the true label outputs the highest score.\n",
    "\n",
    "    Args:\n",
    "    perceptron_weights (list of numpy.ndarray): The list of weights for each perceptron.\n",
    "    X_test (numpy.ndarray): The test set images.\n",
    "    y_test (numpy.ndarray): The true labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "    float: The overall test accuracy.\n",
    "    \"\"\"\n",
    "    perceptron_outputs = np.array([predict_perceptron(weights, X_test) for weights in perceptron_weights])\n",
    "\n",
    "    predicted_classes = np.argmax(perceptron_outputs, axis=0)\n",
    "\n",
    "    overall_accuracy = np.mean(predicted_classes == y_test)\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "overall_test_accuracy = calculate_overall_accuracy(perceptron_weights, test_images, test_labels)\n",
    "\n",
    "print(f'The overall test accuracy is: {overall_test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression (40 points)\n",
    "\n",
    "Logistic regression is a binary classification method which can be modeled as using a single\n",
    "neuron reading in an input vector $ x \\in \\mathbb{R}^d $ and parameterized by weight vector $ w \\in \\mathbb{R}^d $,\n",
    "where the neuron outputs the probability of the class being $ y = 1 $ given $ x $\n",
    "\n",
    "$$\n",
    "P(y = 1|x) = g_w(x) = \\frac{1}{1 + \\exp(-w^T x)} = \\sigma(w^T x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y = 0|x) = 1 - P(y = 1|x) = 1 - g_w(x).\n",
    "$$\n",
    "\n",
    "Given $ (x^{(i)}, y^{(i)})_{i=1}^N $, the Cross Entropy Loss function is defined as follows\n",
    "\n",
    "$$\n",
    "J(w) = -\\sum_{i=1}^N \\left( y^{(i)} \\log(g_w(x^{(i)})) + (1 - y^{(i)}) \\log(1 - g_w(x^{(i)})) \\right),\n",
    "$$\n",
    "\n",
    "where $ N $ denotes the total number of training samples. We will optimize this cost function via gradient descent.\n",
    "\n",
    "1. Show that the gradient of the cost function with respect to the parameter $ w $ is:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial J(w)}{\\partial w_j} = \\sum_{i=1}^N x_j^{(i)} (g_w(x^{(i)}) - y^{(i)}).\n",
    "    $$\n",
    "\n",
    "    Show your work.\n",
    "\n",
    "2. Using the gradient derived for Logistic Regression cross entropy loss, use gradient descent to classify given $ x \\in \\mathbb{R}^{785} $ whether a digit’s label is $ k $ or if it is some \"other\" digit, i.e. $ k \\neq \\{0, \\ldots, 9\\} $. Report the test accuracy for each of the 10 two-way classifications on the test set.\n",
    "\n",
    "3. For each image in the test set, report the overall test accuracy. An example will be considered labeled correctly if the perceptron classification of the true label has the highest probability. So for instance, if the true label was $ \\{2\\} $ for an image, you would count it as correctly classified if the perceptron test of $ \\{2\\} $ vs. $ \\{0,1,3,4,5,6,7,8,9\\} $ had the highest probability of all the 10 2-way classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a310a2040d44441f9f3e9fb5e5631508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training logistic regression models for each digit:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for digit 0 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794cf06ec0e84586a9f2e0a6237fd155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 0: 0.9909\n",
      "Test accuracy for digit 0: 0.9895\n",
      "\n",
      "Training model for digit 1 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d82a19a3cc4477bbd3ca4d94742831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 1: 0.9928\n",
      "Test accuracy for digit 1: 0.9915\n",
      "\n",
      "Training model for digit 2 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c690a64f4694de1bfda70db858e7be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 2: 0.9791\n",
      "Test accuracy for digit 2: 0.9735\n",
      "\n",
      "Training model for digit 3 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da8ffa4fdbe4a5e945e67bcd40f79e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 3: 0.9744\n",
      "Test accuracy for digit 3: 0.9655\n",
      "\n",
      "Training model for digit 4 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6084c976c8478e9a08c184badbf68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 4: 0.9856\n",
      "Test accuracy for digit 4: 0.9745\n",
      "\n",
      "Training model for digit 5 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107397cf33b043dd81d52471a1cb17f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 5: 0.9456\n",
      "Test accuracy for digit 5: 0.9230\n",
      "\n",
      "Training model for digit 6 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3235e623084a4fd799612a08b1e53106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 6: 0.9843\n",
      "Test accuracy for digit 6: 0.9730\n",
      "\n",
      "Training model for digit 7 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2947568dcb24bc69c04a24ddfc9d5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 7: 0.9847\n",
      "Test accuracy for digit 7: 0.9660\n",
      "\n",
      "Training model for digit 8 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88438e0eccb48b3aae63328287c5223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 8: 0.9445\n",
      "Test accuracy for digit 8: 0.9270\n",
      "\n",
      "Training model for digit 9 vs all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b63bf5a7ad74e5b92bf859304214d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gradient Descent Iterations:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for digit 9: 0.9150\n",
      "Test accuracy for digit 9: 0.8965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, epsilon=1e-5):\n",
    "    m = X.shape[0]\n",
    "    predictions = sigmoid(np.dot(X, w))\n",
    "    cost = -(1/m) * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, w, learning_rate, iterations):\n",
    "    m = X.shape[0]\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in tqdm(range(iterations), desc='Gradient Descent Iterations'):\n",
    "        predictions = sigmoid(np.dot(X, w))\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error) / m\n",
    "        w -= learning_rate * gradient\n",
    "        cost_history.append(compute_cost(X, y, w))\n",
    "    \n",
    "    return w, cost_history\n",
    "\n",
    "def predict(X, w):\n",
    "    predictions = sigmoid(np.dot(X, w))\n",
    "    return [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "learning_rate = 0.1\n",
    "iterations = 1000\n",
    "\n",
    "trained_models = []\n",
    "\n",
    "\n",
    "for digit in tqdm(range(10), desc=\"Training logistic regression models for each digit\"):\n",
    "    print(f\"Training model for digit {digit} vs all\")\n",
    "    y_train_binary = (train_labels == digit).astype(int)\n",
    "    y_test_binary = (test_labels == digit).astype(int)\n",
    "\n",
    "    initial_weights = np.zeros(train_images.shape[1])\n",
    "\n",
    "    final_weights, cost_history = gradient_descent(train_images, y_train_binary, initial_weights, learning_rate, iterations)\n",
    "    \n",
    "    trained_models.append(final_weights)\n",
    "    \n",
    "    y_pred_train = predict(train_images, final_weights)\n",
    "    train_accuracy = np.mean(y_train_binary == y_pred_train)\n",
    "    \n",
    "    y_pred_test = predict(test_images, final_weights)\n",
    "    test_accuracy = np.mean(y_test_binary == y_pred_test)\n",
    "    \n",
    "    print(f\"Training accuracy for digit {digit}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy for digit {digit}: {test_accuracy:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall test accuracy is: 0.8105\n"
     ]
    }
   ],
   "source": [
    "def calculate_overall_test_accuracy(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate the overall test accuracy. An image is considered correctly classified\n",
    "    if the model corresponding to the true label gives the highest probability.\n",
    "\n",
    "    Args:\n",
    "    models (list of numpy.ndarray): List of weight vectors for each logistic regression model.\n",
    "    X_test (numpy.ndarray): Test set images.\n",
    "    y_test (numpy.ndarray): True labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "    float: The overall test accuracy.\n",
    "    \"\"\"\n",
    "    probabilities = np.array([sigmoid(np.dot(X_test, w)) for w in models])\n",
    "    \n",
    "    predicted_classes = np.argmax(probabilities, axis=0)\n",
    "    \n",
    "    overall_accuracy = np.mean(predicted_classes == y_test)\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "\n",
    "overall_test_accuracy = calculate_overall_test_accuracy(trained_models, test_images, test_labels)\n",
    "\n",
    "print(f\"The overall test accuracy is: {overall_test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsp2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
